{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Transformer</b>\n",
    "\n",
    "adapted from: https://github.com/zhangxiangnick/Transformer-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tb 2019-01-20 \n",
      "\n",
      "CPython 3.6.4\n",
      "IPython 6.2.1\n",
      "\n",
      "sys 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 12:04:33) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "numpy 1.14.2\n",
      "pandas 0.22.0\n",
      "sklearn 0.19.2\n",
      "torch 1.0.0a0+1e45e7a\n",
      "IPython 6.2.1\n",
      "\n",
      "compiler   : GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)\n",
      "system     : Darwin\n",
      "release    : 17.5.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 24\n",
      "interpreter: 64bit\n",
      "GPU Name: TITAN Xp\n",
      "GPU Memory: 12.0GB\n",
      "CUDA Version: (9, 1, 0)\n",
      "GPU Free/Total Memory: 97%\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import math, copy, time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import Field\n",
    "\n",
    "from lib.checkpoint import *\n",
    "from lib.stopping import Stopping\n",
    "from lib.tools import *\n",
    "from lib.trainlogger import *\n",
    "from lib.utilities import *\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -a \"tb\" -d -v -m -p sys,numpy,pandas,sklearn,torch,IPython\n",
    "gpu_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available = lambda : False\n",
    "# torch.backends.cudnn.enabled=False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = HYPERPARAMETERS({\n",
    "    'EXPERIMENT': 'Eng2Ger',\n",
    "    'DESCRIPTION': 'Transformer model',\n",
    "    'TIMESTAMP': HYPERPARAMETERS.create_timestamp(),\n",
    "\n",
    "    'MODEL_NAME': 'Eng2Ger_TRANSFORMER',\n",
    "\n",
    "    'PRELOAD_MODEL_PATH': None,\n",
    "\n",
    "    'ROOT_DIR': 'data',\n",
    "\n",
    "    'TARGET_ENCODING': 'sts',  # ' ctc\n",
    "\n",
    "    'BATCH_SIZE': 128,\n",
    "    'NUM_WORKERS': 8,\n",
    "\n",
    "    'EMBEDDING_SIZE': 256,\n",
    "    'EMBEDDING_DROPOUT': 0.2,\n",
    "    'RNN_HIDDEN_SIZE': 256,\n",
    "    'RNN_NUM_LAYERS': 2,\n",
    "    'RNN_DROPOUT': 0.2,\n",
    "    'BIDIRECTIONAL': True,\n",
    "\n",
    "    'LR': 0.0003,\n",
    "    'LR_LAMBDA': lambda epoch: max(math.pow(0.78, math.floor((1 + epoch) / 200.0)), 0.01),\n",
    "    'WEIGHT_DECAY': 0,\n",
    "    'MOMENTUM': 0.9,\n",
    "    'NESTEROV': True,\n",
    "\n",
    "    'LABEL_SMOOTHING' : 0.2,\n",
    "\n",
    "    'MAX_GRAD_NORM': 400,\n",
    "\n",
    "    'MAX_EPOCHS': 30,\n",
    "\n",
    "    'STOPPING_PATIENCE': 80,\n",
    "\n",
    "    'CHECKPOINT_INTERVAL': 10,\n",
    "    'CHECKPOINT_RESTORE': False,\n",
    "\n",
    "    'USE_CUDA': torch.cuda.is_available(),\n",
    "\n",
    "    'SEED': 123456,\n",
    "    \n",
    "    'SEQ_MAX_LEN' :         50,\n",
    "    'SRC_VOCAB_MAX_SIZE' :  50000,\n",
    "    'TGT_VOCAB_MAX_SIZE' :  50000,\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(H.SEED)\n",
    "np.random.seed(H.SEED)\n",
    "torch.manual_seed(H.SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(H.SEED)\n",
    "    torch.cuda.manual_seed_all(H.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYM_SOS = '<sos>'\n",
    "SYM_EOS = '<eos>'\n",
    "SYM_PAD = '<pad>'\n",
    "IDX_SOS = -1\n",
    "IDX_EOS = -1\n",
    "IDX_PAD = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_en = spacy.load('en')\n",
    "spacy_de = spacy.load('de')\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer( text )]\n",
    "    return text.split()\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "    return text.split()\n",
    "\n",
    "preproc = lambda seq: [SYM_SOS] + seq + [SYM_EOS]\n",
    "\n",
    "src = Field(sequential=True, tokenize=tokenize_en, lower=True, batch_first=True, \n",
    "            include_lengths=True)\n",
    "tgt = Field(sequential=True, tokenize=tokenize_de, lower=True, batch_first=True, \n",
    "            include_lengths=True, preprocessing=preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_filter(example):\n",
    "    return len(example.src) <= H.SEQ_MAX_LEN and len(example.tgt) <= H.SEQ_MAX_LEN\n",
    "\n",
    "path = os.path.join(H.ROOT_DIR, \"eng-ger-data.tsv\")\n",
    "SRC_FIELD_NAME = 'src'\n",
    "TGT_FIELD_NAME = 'tgt'\n",
    "\n",
    "train_data, valid_data, test_data = torchtext.data.TabularDataset(\n",
    "    path=path, format='tsv',\n",
    "    fields=[(SRC_FIELD_NAME, src), (TGT_FIELD_NAME, tgt)],\n",
    "    filter_pred=len_filter\n",
    "    ).split(split_ratio=[0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __call__(self, val):\n",
    "        if isinstance(val, str):\n",
    "            res = self.vocab.stoi[val] if val in self.vocab.stoi else None\n",
    "        elif isinstance(val, int):\n",
    "            res = self.vocab.itos[val] if val <= self.__len__() else None\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "        return res   \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab.itos)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Vocab(size=' + str(len(self.vocab.itos)) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=9510) Vocab(size=15657)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 3, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.build_vocab(train_data, max_size=H.SRC_VOCAB_MAX_SIZE, min_freq=2)\n",
    "tgt.build_vocab(train_data, max_size=H.TGT_VOCAB_MAX_SIZE, min_freq=2)\n",
    "\n",
    "input_vocab = Vocabulary(src.vocab)\n",
    "output_vocab = Vocabulary(tgt.vocab)\n",
    "\n",
    "print(input_vocab, output_vocab)\n",
    "\n",
    "IDX_PAD = output_vocab(SYM_PAD)\n",
    "IDX_SOS = output_vocab(SYM_SOS)\n",
    "IDX_EOS = output_vocab(SYM_EOS)\n",
    "\n",
    "IDX_PAD, IDX_SOS, IDX_EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1058, 133, 133)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter, valid_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "                                (train_data, valid_data, test_data), \n",
    "                                batch_size=H.BATCH_SIZE, repeat=False, \n",
    "                                sort=False, sort_within_batch=True, \n",
    "                                sort_key=lambda x: len(x.src))\n",
    "\n",
    "\n",
    "batch = next(train_iter.__iter__())\n",
    "input_variables = getattr(batch, 'src')\n",
    "target_variables = getattr(batch, 'tgt')\n",
    "\n",
    "len(train_iter), len(valid_iter), len(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_batch, batch in enumerate(train_iter):\n",
    "    inputs_cpu, input_sizes_cpu = getattr(batch, SRC_FIELD_NAME)\n",
    "    labels_cpu, label_sizes_cpu = getattr(batch, TGT_FIELD_NAME)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, droput, len_max=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.droput = droput\n",
    "        self.len_max = len_max\n",
    "\n",
    "        position = torch.arange(0.0, self.len_max)\n",
    "        num_timescales = self.d_model // 2\n",
    "        log_timescale_increment = math.log(10000) / (num_timescales - 1)\n",
    "        inv_timescales = torch.exp(torch.arange(0.0, num_timescales) * -log_timescale_increment)\n",
    "        scaled_time = position.unsqueeze(1) * inv_timescales.unsqueeze(0)\n",
    "        pos_emb = torch.cat((torch.sin(scaled_time), torch.cos(scaled_time)), 1)\n",
    "\n",
    "        # wrap in a buffer so that model can be moved to GPU\n",
    "        self.register_buffer('pos_emb', pos_emb)\n",
    "\n",
    "        self.drop = nn.Dropout(self.droput)\n",
    "\n",
    "    def forward(self, word_emb):\n",
    "        len_seq = word_emb.size(1)\n",
    "        out = word_emb + self.pos_emb[:len_seq, :]\n",
    "        out = self.drop(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, d_model, droput):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.droput = droput\n",
    "\n",
    "        self.d_head = d_model // self.num_heads\n",
    "\n",
    "        self.fc_query = nn.Linear(self.d_model, self.num_heads * self.d_head, bias=False)\n",
    "        self.fc_key = nn.Linear(self.d_model, self.num_heads * self.d_head, bias=False)\n",
    "        self.fc_value = nn.Linear(self.d_model, self.num_heads * self.d_head, bias=False)\n",
    "\n",
    "        self.fc_concat = nn.Linear(self.num_heads * self.d_head, self.d_model, bias=False)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(self.droput)\n",
    "        self.dropout = nn.Dropout(self.droput)\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.d_model)\n",
    "\n",
    "    def _prepare_proj(self, x):\n",
    "        \"\"\"Reshape the projectons to apply softmax on each head\n",
    "        \"\"\"\n",
    "        b, l, d = x.size()\n",
    "        return x.view(b, l, self.num_heads, self.d_head).transpose(1, 2).contiguous().view(b * self.num_heads, l,\n",
    "                                                                                           self.d_head)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        b, len_query = query.size(0), query.size(1)\n",
    "        len_key = key.size(1)\n",
    "\n",
    "        # project inputs to multi-heads\n",
    "        proj_query = self.fc_query(query)  # batch_size x len_query x h*d_head\n",
    "        proj_key = self.fc_key(key)  # batch_size x len_key x h*d_head\n",
    "        proj_value = self.fc_value(value)  # batch_size x len_key x h*d_head\n",
    "\n",
    "        # prepare the shape for applying softmax\n",
    "        proj_query = self._prepare_proj(proj_query)  # batch_size*h x len_query x d_head\n",
    "        proj_key = self._prepare_proj(proj_key)  # batch_size*h x len_key x d_head\n",
    "        proj_value = self._prepare_proj(proj_value)  # batch_size*h x len_key x d_head\n",
    "\n",
    "        # get dotproduct softmax attns for each head\n",
    "        attns = torch.bmm(proj_query, proj_key.transpose(1, 2))  # batch_size*h x len_query x len_key\n",
    "        attns = attns / math.sqrt(self.d_head)\n",
    "        attns = attns.view(b, self.num_heads, len_query, len_key)\n",
    "        attns = attns.masked_fill_(mask.unsqueeze(1), -float('inf'))\n",
    "        attns = self.softmax(attns.view(-1, len_key))\n",
    "\n",
    "        # return mean attention from all heads as coverage\n",
    "        coverage = torch.mean(attns.view(b, self.num_heads, len_query, len_key), dim=1)\n",
    "\n",
    "        attns = self.attn_dropout(attns)\n",
    "        attns = attns.view(b * self.num_heads, len_query, len_key)\n",
    "\n",
    "        # apply attns on value\n",
    "        out = torch.bmm(attns, proj_value)  # batch_size*h x len_query x d_head\n",
    "        out = out.view(b, self.num_heads, len_query, self.d_head).transpose(1, 2).contiguous()\n",
    "\n",
    "        out = self.fc_concat(out.view(b, len_query, self.num_heads * self.d_head))\n",
    "\n",
    "        out = self.dropout(out).add_(query)\n",
    "        out = self.norm(out)\n",
    "        return out, coverage\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        self.drop = nn.Dropout(self.dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.fc(inputs)\n",
    "        out = self.drop(out).add_(inputs)\n",
    "        out = self.norm(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads, d_model, dropout, d_ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention = MultiHeadAttention(self.num_heads, self.d_model, self.dropout)\n",
    "\n",
    "        self.ff = PositionwiseFeedForward(self.d_model, self.d_ff, self.dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        out, _ = self.attention(query, key, value, mask)\n",
    "        out = self.ff(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads, d_model, dropout, d_ff):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention_tgt = MultiHeadAttention(self.num_heads, self.d_model, self.dropout)\n",
    "\n",
    "        self.attention_src = MultiHeadAttention(self.num_heads, self.d_model, self.dropout)\n",
    "\n",
    "        self.ff = PositionwiseFeedForward(d_model, self.d_ff, self.dropout)\n",
    "\n",
    "    def forward(self, query, key, value, context, mask_tgt, mask_src):\n",
    "        out, _ = self.attention_tgt(query, key, value, mask_tgt)\n",
    "        out, coverage = self.attention_src(out, context, context, mask_src)\n",
    "        out = self.ff(out)\n",
    "        return out, coverage\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, num_heads, d_model, dropout, d_ff, num_layers=6, padding_idx=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.padding_idx = padding_idx\n",
    "        self.num_layers = num_layers\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.d_model, padding_idx=self.padding_idx)\n",
    "\n",
    "        self.pos_emb = PositionalEncoding(self.d_model, self.dropout, len_max=512)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(self.num_heads, self.d_model, self.dropout, self.d_ff) for _ in range(self.num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        context = self.embeddings(src)  # batch_size x len_src x d_model\n",
    "\n",
    "        context = self.pos_emb(context)\n",
    "\n",
    "        mask_src = src.data.eq(self.padding_idx).unsqueeze(1)\n",
    "        for _, layer in enumerate(self.layers):\n",
    "            context = layer(context, context, context, mask_src)  # batch_size x len_src x d_model\n",
    "        return context, mask_src\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, num_heads, d_model, dropout, d_ff, num_layers=6, padding_idx=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.padding_idx = padding_idx\n",
    "        self.num_layers = num_layers\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_model, padding_idx=self.padding_idx)\n",
    "\n",
    "        self.pos_emb = PositionalEncoding(self.d_model, self.dropout, len_max=512)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(self.num_heads, self.d_model, self.dropout, self.d_ff) for _ in range(self.num_layers)]\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(self.d_model, self.vocab_size, bias=True)\n",
    "\n",
    "        # tie weight between word embedding and generator\n",
    "        self.fc.weight = self.embedding.weight\n",
    "\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        # pre-save a mask to avoid future information in self-attentions in decoder\n",
    "        # save as a buffer, otherwise will need to recreate it and move to GPU during every call\n",
    "        mask = torch.ByteTensor(np.triu(np.ones((self.d_model, self.d_model)), k=1).astype('uint8'))\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, tgt, context, mask_src):\n",
    "        out = self.embedding(tgt)  # batch_size x len_tgt x d_model\n",
    "\n",
    "        out = self.pos_emb(out)\n",
    "\n",
    "        len_tgt = tgt.size(1)\n",
    "        mask_tgt = tgt.data.eq(self.padding_idx).unsqueeze(1) + self.mask[:len_tgt, :len_tgt]\n",
    "        mask_tgt = torch.gt(mask_tgt, 0)\n",
    "        for _, layer in enumerate(self.layers):\n",
    "            out, coverage = layer(out, out, out, context, mask_tgt, mask_src)  # batch_size x len_tgt x d_model\n",
    "\n",
    "        out = self.fc(out)  # batch_size x len_tgt x bpe_size\n",
    "\n",
    "        out = self.logsoftmax(out.view(-1, self.vocab_size))\n",
    "        return out, coverage\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, num_heads, d_model, dropout, d_ff, num_layers=6, padding_idx=1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_vocab = src_vocab\n",
    "        self.src_vocab_size = len(src_vocab)\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.tgt_vocab_size = len(tgt_vocab)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        self.encode = Encoder(self.src_vocab_size, self.num_heads, self.d_model, self.dropout, self.d_ff,\n",
    "                              self.num_layers, self.padding_idx)\n",
    "        self.decode = Decoder(self.tgt_vocab_size, self.num_heads, self.d_model, self.dropout, self.d_ff,\n",
    "                              self.num_layers, self.padding_idx)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        context, mask_src = self.encode(src)\n",
    "        outputs, _ = self.decode(tgt, context, mask_src)\n",
    "\n",
    "        probas = outputs.view(src.size(0), -1, self.tgt_vocab_size)\n",
    "        _, max_indices = torch.max(probas, 2)\n",
    "        proba_sizes = torch.max(max_indices.eq(self.tgt_vocab('<eos>')), dim=1)[1] + 1\n",
    "\n",
    "        return probas, proba_sizes\n",
    "\n",
    "    def decode_greedy(self, inputs, labels=None, max_seq_length=50):\n",
    "\n",
    "        idx_sos, idx_eos = self.tgt_vocab('<sos>'), self.tgt_vocab('<eos>')\n",
    "\n",
    "        context, mask_src = self.encode(inputs)\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        decode_input = torch.ones(batch_size, 1).fill_(idx_sos).type_as(inputs)\n",
    "\n",
    "        dec_output_sizes = torch.LongTensor(batch_size).fill_(max_seq_length).type_as(inputs)\n",
    "\n",
    "        max_steps = labels.size(1) if labels is not None else max_seq_length + 1\n",
    "\n",
    "        dec_outputs = []\n",
    "        for step in range(max_steps):\n",
    "            outputs, _ = self.decode(decode_input, context, mask_src)\n",
    "            outputs = outputs.view(batch_size, -1, self.tgt_vocab_size)\n",
    "\n",
    "            dec_outputs.append(outputs[:, step, :].unsqueeze(1))\n",
    "\n",
    "            preds = torch.max(outputs[:, -1, :], dim=1)[1]\n",
    "\n",
    "            dec_output_sizes[preds.eq(idx_eos) * dec_output_sizes.gt(step)] = step\n",
    "            if labels is None and dec_output_sizes.le(step + 1).all():\n",
    "                break\n",
    "\n",
    "            decode_input = torch.cat([decode_input, preds.unsqueeze(1)], dim=1)\n",
    "\n",
    "        dec_outputs = torch.cat(dec_outputs, dim=1)\n",
    "\n",
    "        return dec_outputs, dec_output_sizes\n",
    "\n",
    "    def decode_beam(self, inputs, labels=None, max_seq_length=50, beam_size=64, alpha=0.1, beta=0.3):\n",
    "\n",
    "        context, mask_src = self.encode(inputs)\n",
    "\n",
    "        max_seq_len = labels.size(1) if labels is not None else max_seq_length\n",
    "\n",
    "        dec_outputs = []\n",
    "        for idx in range(context.size(0)):\n",
    "            target, _ = beam_search(self, self.tgt_vocab, context[idx].unsqueeze(0), mask_src[idx].unsqueeze(0),\n",
    "                                    beam_size=beam_size, alpha=alpha, beta=beta, max_seq_len=max_seq_len)\n",
    "            dec_outputs.append(target)\n",
    "\n",
    "        return dec_outputs\n",
    "\n",
    "\n",
    "def beam_search(model, vocab, context, mask_src, beam_size=64, alpha=0.1, beta=0.3, max_seq_len=64):\n",
    "    probas = []\n",
    "    preds = []\n",
    "    probs = []\n",
    "    coverage_penalties = []\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    idx_sos, idx_eos, idx_pad = vocab('<sos>'), vocab('<eos>'), vocab('<pad>')\n",
    "\n",
    "    decode_inputs = torch.LongTensor([idx_sos]).unsqueeze(1)\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        decode_inputs = decode_inputs.cuda()\n",
    "\n",
    "    decode_outputs, coverage = model.decode(decode_inputs, context, mask_src)\n",
    "\n",
    "    scores, scores_idx = decode_outputs.view(-1).topk(beam_size)\n",
    "    beam_idx = scores_idx / vocab_size\n",
    "    pred_idx = (scores_idx - beam_idx * vocab_size).view(beam_size, -1)\n",
    "\n",
    "    decode_inputs = torch.cat((decode_inputs.repeat(beam_size, 1), pred_idx), 1)\n",
    "    context = context.repeat(beam_size, 1, 1)\n",
    "\n",
    "    remaining_beams = beam_size\n",
    "    for step in range(max_seq_len):\n",
    "        decode_outputs, coverage = model.decode(decode_inputs, context, mask_src)\n",
    "\n",
    "        decode_outputs = decode_outputs.view(remaining_beams, -1, vocab_size)\n",
    "        decode_outputs = scores.unsqueeze(1) + decode_outputs[:, -1, :]\n",
    "        scores, scores_idx = decode_outputs.view(-1).topk(remaining_beams)\n",
    "\n",
    "        beam_idx = scores_idx / vocab_size\n",
    "        pred_idx = (scores_idx - beam_idx * vocab_size).view(remaining_beams, -1)\n",
    "\n",
    "        decode_inputs = torch.cat((decode_inputs[beam_idx], pred_idx), 1)\n",
    "\n",
    "        index = decode_inputs[:, -1].eq(idx_eos) + decode_inputs[:, -1].eq(idx_pad)\n",
    "        finished = index.nonzero().flatten()\n",
    "        continue_idx = (index ^ 1).nonzero().flatten()\n",
    "\n",
    "        for idx in finished:\n",
    "            probas.append(scores[idx].item())\n",
    "            preds.append(decode_inputs[idx, :].tolist())\n",
    "            probs.append(coverage[idx, :, :])\n",
    "\n",
    "            atten_prob = torch.sum(coverage[idx, :, :], dim=0)\n",
    "            coverage_penalty = torch.log(atten_prob.masked_select(atten_prob.le(1)))\n",
    "            coverage_penalty = beta * torch.sum(coverage_penalty).item()\n",
    "            coverage_penalties.append(coverage_penalty)\n",
    "\n",
    "            remaining_beams -= 1\n",
    "\n",
    "        if len(continue_idx) > 0:\n",
    "            scores = scores.index_select(0, continue_idx)\n",
    "            decode_inputs = decode_inputs.index_select(0, continue_idx)\n",
    "            context = context.index_select(0, continue_idx)\n",
    "\n",
    "        if remaining_beams <= 0:\n",
    "            break\n",
    "\n",
    "    len_penalties = [math.pow(len(pred), alpha) for pred in preds]\n",
    "    #     final_scores = [probas[i] / len_penalties[i] + coverage_penalties[i] for i in range(len(preds))]\n",
    "    final_scores = [probas[i] / len_penalties[i] for i in range(len(preds))]\n",
    "\n",
    "    sorted_scores_arg = sorted(range(len(preds)), key=lambda i: -final_scores[i])\n",
    "\n",
    "    best_beam = sorted_scores_arg[0]\n",
    "\n",
    "    return preds[best_beam], probs[best_beam]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cpu = Transformer(input_vocab, output_vocab, num_heads=8, d_model=512, \n",
    "                        dropout=0.1, d_ff=1024, num_layers=6, padding_idx=IDX_PAD)\n",
    "\n",
    "outputs_cpu, output_sizes_cpu = model_cpu(inputs_cpu, labels_cpu)\n",
    "\n",
    "# print(outputs_cpu.shape, output_sizes_cpu.shape)\n",
    "\n",
    "# outputs_cpu, output_sizes_cpu = model_cpu.decode_greedy(inputs_cpu, labels_cpu)\n",
    "\n",
    "# print(outputs_cpu.shape, output_sizes_cpu.shape)\n",
    "\n",
    "# outputs_cpu, output_sizes_cpu = model_cpu.decode_beam(inputs_cpu, labels_cpu)\n",
    "\n",
    "# print(outputs_cpu.shape, output_sizes_cpu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STSDecoder(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_labels(labels, label_sizes, vocab):\n",
    "        lseq = []\n",
    "        for seq, size in zip(labels, label_sizes):\n",
    "            lseq.append(' '.join([vocab(c.item()) for c in seq[0:size - 1]]))\n",
    "\n",
    "        return lseq\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_probas(probas, probas_sizes, vocab, probabilities=False):\n",
    "        max_vals, max_indices = torch.max(probas, 2)\n",
    "\n",
    "        decoded_seq = []\n",
    "        for seq_idx, seq_len, seq_proba in zip(max_indices.cpu(), probas_sizes, max_vals):\n",
    "            txt, probas = '', []\n",
    "\n",
    "            for i in range(min(seq_len, len(seq_idx))):\n",
    "                txt += vocab(seq_idx[i].item()) + ' '\n",
    "                probas.append(math.exp(seq_proba[i].item()))\n",
    "\n",
    "            if probabilities:\n",
    "                decoded_seq.append((txt.strip(), stats.mean(probas) if len(probas) > 0 else 0))\n",
    "            else:\n",
    "                decoded_seq.append(txt.strip())\n",
    "        return decoded_seq\n",
    "\n",
    "    def __call__(self, inputs, inputs_sizes, labels=None, label_sizes=None, probabilities=False):\n",
    "\n",
    "        decoder_seq = self.decode_probas(inputs, inputs_sizes, self.vocab, probabilities=probabilities)\n",
    "\n",
    "        label_seq = None\n",
    "        if labels is not None and label_sizes is not None:\n",
    "            label_seq = self.decode_labels(labels, label_sizes, self.vocab)\n",
    "\n",
    "        return decoder_seq, label_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, padding_idx, label_smoothing=0.0):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - label_smoothing\n",
    "        self.smoothing = label_smoothing\n",
    "\n",
    "    def __call__(self, inputs, input_sizes, labels, label_sizes):\n",
    "        return self.forward(inputs, input_sizes, labels, label_sizes)\n",
    "\n",
    "    def forward(self, inputs, input_sizes, labels, label_sizes):\n",
    "        b, t, c = inputs.size()\n",
    "        inputs = inputs.view(b * t, c)\n",
    "\n",
    "        b, t = labels.size()\n",
    "        labels = labels.view(b * t)\n",
    "\n",
    "        true_dist = inputs.clone()\n",
    "        true_dist.fill_(self.smoothing / (inputs.size(1) - 2))\n",
    "        true_dist.scatter_(1, labels.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "\n",
    "        mask = torch.nonzero(labels == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "\n",
    "        return self.criterion(inputs, true_dist.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.scorer import Scorer\n",
    "from lib.stopping import Stopping\n",
    "\n",
    "m = Metric([('train_loss', np.inf), ('train_score', np.inf), ('valid_loss', np.inf), ('valid_score', 0),\n",
    "            ('train_lr', 0), ('valid_cer', np.inf)])\n",
    "\n",
    "model = Transformer(input_vocab, output_vocab, num_heads=8, d_model=512, \n",
    "                        dropout=0.1, d_ff=1024, num_layers=6, padding_idx=IDX_PAD)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        torch_weight_init(p)\n",
    "\n",
    "if H.USE_CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "logging.info(model_summary(model, line_length=100))\n",
    "\n",
    "# if H.PRELOAD_MODEL_PATH:\n",
    "#     path = os.path.join(H.EXPERIMENT, H.PRELOAD_MODEL_PATH)\n",
    "#     state = torch.load(path)\n",
    "#     model.load_state_dict(state)\n",
    "#     logging.info(\"Preloaded model: {}\".format(path))\n",
    "\n",
    "if H.PRELOAD_MODEL_PATH:\n",
    "    path = os.path.join(H.EXPERIMENT, H.PRELOAD_MODEL_PATH)\n",
    "    state = torch.load(path)\n",
    "    model.load_state_dict(state)\n",
    "    logging.info(\"Preloaded model: {}\".format(path))    \n",
    "    \n",
    "    \n",
    "criterion = LabelSmoothingLoss(padding_idx=0, label_smoothing=H.LABEL_SMOOTHING)\n",
    "\n",
    "sts_decoder = STSDecoder(output_vocab)\n",
    "\n",
    "scorer = Scorer()\n",
    "\n",
    "optimizer = optim.Adam(list(filter(lambda p: p.requires_grad, model.parameters())),\n",
    "                       amsgrad=False,\n",
    "                       betas=(0.9, 0.999),\n",
    "                       eps=1e-08,\n",
    "                       lr=H.LR,\n",
    "                       weight_decay=H.WEIGHT_DECAY)\n",
    "\n",
    "stopping = Stopping(model, patience=H.STOPPING_PATIENCE)\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[H.LR_LAMBDA])\n",
    "\n",
    "tlogger = TensorboardLogger(root_dir=H.EXPERIMENT, experiment_dir=H.TIMESTAMP)  # PytorchLogger()\n",
    "\n",
    "checkpoint = Checkpoint(model, optimizer, stopping, m,\n",
    "                        root_dir=H.EXPERIMENT, experiment_dir=H.TIMESTAMP, restore_from=-1,\n",
    "                        interval=H.CHECKPOINT_INTERVAL, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_start = 1\n",
    "if H.CHECKPOINT_RESTORE:\n",
    "    epoch_start = checkpoint.restore() + 1\n",
    "#     train_loader.batch_sampler.shuffle(epoch_start)\n",
    "\n",
    "epoch = epoch_start\n",
    "try:\n",
    "    epoch_itr = tlogger.set_itr(range(epoch_start, H.MAX_EPOCHS + 1))\n",
    "\n",
    "    for epoch in epoch_itr:\n",
    "        \n",
    "#         with DelayedKeyboardInterrupt():\n",
    "\n",
    "        model.train(True)\n",
    "\n",
    "        scheduler.step()\n",
    "    \n",
    "        train_lr = [float(param_group['lr']) for param_group in optimizer.param_groups][0]\n",
    "\n",
    "        total_size, total_loss, total_score = 0, 0.0, 0.0\n",
    "        for idx_batch, batch in enumerate(train_iter):\n",
    "            inputs, input_sizes = getattr(batch, SRC_FIELD_NAME)\n",
    "            labels, label_sizes = getattr(batch, TGT_FIELD_NAME)\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            probas, proba_sizes = model(inputs, labels[:, :-1])\n",
    "\n",
    "            loss = criterion(probas, proba_sizes, labels[:,1:].contiguous(), label_sizes-1)\n",
    "            total_loss += loss.item()      \n",
    "            \n",
    "#             preds_seq, label_seq = sts_decoder(probas, proba_sizes, labels.contiguous(), label_sizes)\n",
    "#             total_score += scorer(preds_seq, label_seq)\n",
    "            total_score += 1\n",
    "    \n",
    "            total_size += inputs.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            if H.MAX_GRAD_NORM is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), H.MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "\n",
    "            del probas\n",
    "            del loss\n",
    "            \n",
    "        m.train_loss = total_loss / total_size\n",
    "        m.train_score = 1.0 - min(1.0, total_score / total_size)\n",
    "        m.train_lr = train_lr\n",
    "    \n",
    "        #-----------------------------------------------------------\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            hypotheses = []\n",
    "            references = []\n",
    "            total_size, total_loss, total_score = 0, 0.0, 0.0\n",
    "            for idx_batch, batch in enumerate(valid_iter):\n",
    "                inputs, input_sizes = getattr(batch, SRC_FIELD_NAME)\n",
    "                labels, label_sizes = getattr(batch, TGT_FIELD_NAME)\n",
    "                if next(model.parameters()).is_cuda:\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                probas, proba_sizes = model.decode_greedy(inputs, labels[:,1:], H.SEQ_MAX_LEN)\n",
    "                \n",
    "#                 loss = criterion(probas, proba_sizes, labels[:,1:].contiguous(), label_sizes-1)\n",
    "#                 total_loss += loss.item()      \n",
    "                total_loss += 1\n",
    "\n",
    "                preds_seq, label_seq = sts_decoder(probas, proba_sizes, labels[:,1:].contiguous(), label_sizes-1)\n",
    "                total_score += scorer(preds_seq, label_seq)\n",
    "\n",
    "                total_size += inputs.size(0)\n",
    "                \n",
    "            del probas\n",
    "#             del loss\n",
    "\n",
    "        m.valid_loss = total_loss / total_size\n",
    "        m.valid_score = 1.0 - min(1.0, total_score / total_size)\n",
    "\n",
    "        if checkpoint:\n",
    "            checkpoint.step(epoch)\n",
    "\n",
    "        stopping_flag = stopping.step(epoch, m.valid_loss, m.valid_score)\n",
    "\n",
    "        epoch_itr.log_values(m.train_loss, m.train_score, m.train_lr, m.valid_loss, m.valid_score,\n",
    "                             stopping.best_score_epoch, stopping.best_score)\n",
    "\n",
    "        if stopping_flag:\n",
    "            logger.info(\n",
    "                \"Early stopping at epoch: %d, score %f\" % (stopping.best_score_epoch, stopping.best_score))\n",
    "            break\n",
    "\n",
    "#             train_loader.batch_sampler.shuffle(epoch)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.info(\"Training interrupted at: {}\".format(epoch))\n",
    "    pass\n",
    "\n",
    "checkpoint.create(epoch)\n",
    "\n",
    "model.load_state_dict(stopping.best_score_state)\n",
    "torch.save(model.state_dict(), os.path.join(H.EXPERIMENT, H.MODEL_NAME + '.tar'))\n",
    "\n",
    "logger.info(repr(tlogger))\n",
    "logger.info(repr(stopping))\n",
    "logger.info(repr(checkpoint))\n",
    "\n",
    "logger.info(\"Training end.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pre = Transformer(input_vocab, output_vocab, num_heads=8, d_model=512, \n",
    "                        dropout=0.1, d_ff=1024, num_layers=6, padding_idx=IDX_PAD)\n",
    "if H.USE_CUDA:\n",
    "    model_pre.cuda()\n",
    "\n",
    "path = os.path.join(H.EXPERIMENT, 'Eng2Ger_TRANSFORMER' + '.tar')\n",
    "state = torch.load(path)\n",
    "model_pre.load_state_dict(state)\n",
    "\n",
    "scorer = Scorer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.2 s, sys: 217 ms, total: 43.4 s\n",
      "Wall time: 43.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "model_pre.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    hypotheses = []\n",
    "    references = []\n",
    "    for idx_batch, batch in enumerate(test_iter):\n",
    "        inputs, input_sizes = getattr(batch, SRC_FIELD_NAME)\n",
    "        labels, label_sizes = getattr(batch, TGT_FIELD_NAME)\n",
    "        if next(model_pre.parameters()).is_cuda:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        probas, proba_sizes = model_pre.decode_greedy( inputs, labels[:,1:], H.SEQ_MAX_LEN)\n",
    "        \n",
    "        preds_seq, label_seq = sts_decoder(probas, proba_sizes, labels[:,1:].contiguous(), label_sizes-1)\n",
    "\n",
    "        hypotheses.extend(preds_seq)\n",
    "        references.extend(label_seq)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \n",
      "Bleu: 43.570\n",
      "WER:  35.392\n",
      "CER:  33.750\n",
      "ACC:  19.818\n"
     ]
    }
   ],
   "source": [
    "from lib.scorer import Scorer\n",
    "\n",
    "bleu = Scorer.get_moses_multi_bleu(hypotheses, references, lowercase=False)\n",
    "wer, cer = Scorer.get_wer_cer(hypotheses, references)\n",
    "acc = Scorer.get_acc(hypotheses, references)\n",
    "\n",
    "\n",
    "print('Test Summary \\n'\n",
    "            'Bleu: {bleu:.3f}\\n'\n",
    "            'WER:  {wer:.3f}\\n'\n",
    "            'CER:  {cer:.3f}\\n'\n",
    "            'ACC:  {acc:.3f}'.format(bleu=bleu, wer=wer * 100, cer=cer * 100, acc=acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40min 1s, sys: 3.44 s, total: 40min 5s\n",
      "Wall time: 40min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_pre.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    hypotheses = []\n",
    "    references = []\n",
    "    for idx_batch, batch in enumerate(test_iter):\n",
    "        inputs, input_sizes = getattr(batch, SRC_FIELD_NAME)\n",
    "        labels, label_sizes = getattr(batch, TGT_FIELD_NAME)\n",
    "        if next(model.parameters()).is_cuda:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        context, mask_src = model_pre.encode(inputs)\n",
    "        \n",
    "        max_seq_len = labels.size(1) if labels is not None else H.MAX_SEQ_LENGTH\n",
    "\n",
    "        outputs = model_pre.decode_beam(inputs, labels[:,1:], max_seq_len,\n",
    "                                         beam_size=20, alpha=0.1, beta=0.3)\n",
    "\n",
    "        for entry in outputs:\n",
    "            hypotheses.append(' '.join([output_vocab(t) for t in entry if t not in [IDX_PAD, IDX_SOS, IDX_EOS]]))\n",
    "\n",
    "        references.extend(STSDecoder.decode_labels(labels[:,1:], label_sizes-1, output_vocab)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \n",
      "Bleu: 43.570\n",
      "WER:  34.519\n",
      "CER:  33.098\n",
      "ACC:  20.598\n"
     ]
    }
   ],
   "source": [
    "from lib.scorer import Scorer\n",
    "\n",
    "bleu = Scorer.get_moses_multi_bleu(hypotheses, references, lowercase=False)\n",
    "wer, cer = Scorer.get_wer_cer(hypotheses, references)\n",
    "acc = Scorer.get_acc(hypotheses, references)\n",
    "\n",
    "\n",
    "print('Test Summary \\n'\n",
    "            'Bleu: {bleu:.3f}\\n'\n",
    "            'WER:  {wer:.3f}\\n'\n",
    "            'CER:  {cer:.3f}\\n'\n",
    "            'ACC:  {acc:.3f}'.format(bleu=bleu, wer=wer * 100, cer=cer * 100, acc=acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type in a source sequence:I am at home.\n",
      ">>  I am at home.\n",
      "['i', 'am', 'at', 'home', '.']\n",
      "<<  ich bin zu hause .\n",
      "Type in a source sequence:\n",
      ">>  \n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    seq_str = input(\"Type in a source sequence:\")\n",
    "    print(\">> \", seq_str)\n",
    "    if not len(seq_str):\n",
    "        break\n",
    "    #seq = seq_str.strip().lower().split()\n",
    "    seq = tokenize_en(seq_str.strip().lower())\n",
    "    print(seq)\n",
    "\n",
    "    seq_id = [input_vocab(tok) for tok in seq]\n",
    "\n",
    "    model_pre.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        src_id_seq = torch.LongTensor(seq_id).view(1, -1)\n",
    "        src_id_seq = src_id_seq.cuda() if torch.cuda.is_available() else src_id_seq\n",
    "        \n",
    "        probas, proba_sizes = model_pre.decode_greedy( src_id_seq, labels=None)\n",
    "\n",
    "        tgt_seq = STSDecoder.decode_probas(probas, proba_sizes, output_vocab)\n",
    "        \n",
    "        print(\"<< \", ' '.join(tgt_seq))\n",
    "\n",
    "print(\"Finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
